{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 17:29:43 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:D8:00.0 Off |                    0 |\n",
      "|  0%   45C    P0    91W / 300W |  15720MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       486      C   /pkgs/anaconda37/bin/python     15246MiB |\n",
      "|    0   N/A  N/A     29068      C   /pkgs/anaconda37/bin/python       472MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from UVG1 import UVG\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "from ssf_model import ScaleSpaceFlow\n",
    "#from hific.src.model import Model\n",
    "import numpy as np\n",
    "def load_ssf_model(model, pre_path):\n",
    "    model.motion_encoder.load_state_dict(torch.load(pre_path+'/m_enc.pth'))\n",
    "    model.motion_decoder.load_state_dict(torch.load(pre_path+'/m_dec.pth'))\n",
    "    model.P_encoder.load_state_dict(torch.load(pre_path+'/p_enc.pth'))\n",
    "    model.res_encoder.load_state_dict(torch.load(pre_path+'/r_enc.pth'))\n",
    "    model.res_decoder.load_state_dict(torch.load(pre_path+'/r_dec.pth'))\n",
    "    return model\n",
    "\n",
    "def hwc_tonp(x):\n",
    "    x = x.detach().cpu().numpy()\n",
    "    x = x.transpose([0,2,3,1])\n",
    "    return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else cpu)\n",
    "from torchvision.transforms.functional import resize\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define usefull paths\n",
    "save_path = '/scratch/ssd004/scratch/joaodick/rebuttal_data/UVG_FT/'\n",
    "sequence_number = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.RandomCrop(256)]\n",
    "    )\n",
    "\n",
    "uvg_dataset = UVG(\"./data/uvg/\", train_transforms)\n",
    "uvg_dataloader = DataLoader(\n",
    "        uvg_dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_AR = 0.08\n",
    "ssf_JD  = ScaleSpaceFlow().to(device)\n",
    "ssf_JD.load_state_dict(torch.load('./saved_models/vimeo-90k/JD/ssf_uvg_JD.pth'))\n",
    "ssf_AR = ScaleSpaceFlow().to(device)\n",
    "ssf_AR = load_ssf_model(ssf_AR, f'./saved_models/vimeo-90k/AR_{l_AR}/')\n",
    "ssf_AR_FT_1 = ScaleSpaceFlow().to(device)\n",
    "ssf_AR_FT_1 = load_ssf_model(ssf_AR_FT_1, f'./saved_models/vimeo-90k/AR_FT_1_epoch_MSE/')\n",
    "ssf_AR_FT_2 = ScaleSpaceFlow().to(device)\n",
    "ssf_AR_FT_2 = load_ssf_model(ssf_AR_FT_2, f'./saved_models/vimeo-90k/AR_FT_2_epochs_MSE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "for i in range(sequence_number):\n",
    "    with torch.no_grad():\n",
    "        data = next(iter(uvg_dataloader))\n",
    "        x1 = 2*(data[:,0,...]-0.5)\n",
    "        x2 = 2*(data[:,1,...]-0.5)\n",
    "        x3 = 2*(data[:,2,...]-0.5)\n",
    "        x1_hat = 2*(data[:,3,...] -0.5)\n",
    "        \n",
    "        #RUN INFERENCE\n",
    "        x1=x1.to(device)\n",
    "        x2=x2.to(device)\n",
    "        x3=x3.to(device)\n",
    "        \n",
    "        x1_hat = x1_hat.to(device)\n",
    "        \n",
    "        x2_hat_JD = ssf_JD([x1_hat, x2])\n",
    "        x2_hat_AR = ssf_AR([x1_hat, x2])\n",
    "        x2_hat_AR_FT_1 = ssf_AR_FT_1([x1_hat, x2])\n",
    "        x2_hat_AR_FT_2 = ssf_AR_FT_2([x1_hat, x2])\n",
    "        \n",
    "        x3_hat_JD = ssf_JD([x2_hat_JD, x3])\n",
    "        x3_hat_AR = ssf_AR([x2_hat_AR, x3])\n",
    "        x3_hat_AR_FT_1 = ssf_AR_FT_1([x2_hat_AR, x3])\n",
    "        x3_hat_AR_FT_2 = ssf_AR_FT_2([x2_hat_AR, x3])\n",
    "        \n",
    "        #TRANSFORM BACK TO IMG DOMAIN\n",
    "        x1_img = (hwc_tonp((resize(x1,(128,128))+1)*0.5))[0]\n",
    "        x2_img = (hwc_tonp((resize(x2,(128,128))+1)*0.5))[0]\n",
    "        x3_img = (hwc_tonp((resize(x3,(128,128))+1)*0.5))[0]\n",
    "        \n",
    "        x1_hat_img = (hwc_tonp((resize(x1_hat,(128,128))+1)*0.5))[0]\n",
    "        \n",
    "        x2_hat_JD_img = (hwc_tonp((resize(x2_hat_JD,(128,128))+1)*0.5))[0]\n",
    "        x2_hat_AR_img = (hwc_tonp((resize(x2_hat_AR,(128,128))+1)*0.5))[0]\n",
    "        x2_hat_AR_FT_1_img = (hwc_tonp((resize(x2_hat_AR_FT_1,(128,128))+1)*0.5))[0]\n",
    "        x2_hat_AR_FT_2_img = (hwc_tonp((resize(x2_hat_AR_FT_2,(128,128))+1)*0.5))[0]\n",
    "        \n",
    "        x3_hat_JD_img = (hwc_tonp((resize(x3_hat_JD,(128,128))+1)*0.5))[0]\n",
    "        x3_hat_AR_img = (hwc_tonp((resize(x3_hat_AR,(128,128))+1)*0.5))[0]\n",
    "        x3_hat_AR_FT_1_img = (hwc_tonp((resize(x3_hat_AR_FT_1,(128,128))+1)*0.5))[0]\n",
    "        x3_hat_AR__FT_2_img = (hwc_tonp((resize(x3_hat_AR_FT_2,(128,128))+1)*0.5))[0]\n",
    "        \n",
    "        #create directories\n",
    "        os.makedirs(save_path + f'original/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'JD/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'AR/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'AR_FT_1/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'AR_FT_2/{i}/', exist_ok=True)\n",
    "        #save images\n",
    "        #original\n",
    "        np.save(save_path + f'original/{i}/x1.npy', x1_img)\n",
    "        np.save(save_path + f'original/{i}/x2.npy', x2_img)\n",
    "        np.save(save_path + f'original/{i}/x3.npy', x3_img)\n",
    "        #JD\n",
    "        np.save(save_path + f'JD/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'JD/{i}/x2.npy', x2_hat_JD_img)\n",
    "        np.save(save_path + f'JD/{i}/x3.npy', x3_hat_JD_img)\n",
    "        #AR\n",
    "        np.save(save_path + f'AR/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'AR/{i}/x2.npy', x2_hat_AR_img)\n",
    "        np.save(save_path + f'AR/{i}/x3.npy', x3_hat_AR_img)\n",
    "        #AR_FT_1\n",
    "        np.save(save_path + f'AR_FT_1/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'AR_FT_1/{i}/x2.npy', x2_hat_AR_FT_1_img)\n",
    "        np.save(save_path + f'AR_FT_1/{i}/x3.npy', x3_hat_AR_FT_1_img)\n",
    "        #AR_FT_2\n",
    "        np.save(save_path + f'AR_FT_2/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'AR_FT_2/{i}/x2.npy', x2_hat_AR_FT_2_img)\n",
    "        np.save(save_path + f'AR_FT_2/{i}/x3.npy', x3_hat_AR__FT_2_img)\n",
    "        \n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/ssd004/scratch/joaodick/rebuttal_data/UVG/'\n",
    "os.makedirs(path + 'original_png/', exist_ok=True)\n",
    "for i in range(sequence_number):\n",
    "    x1 = np.load(path + f'original/{i}/x1.npy')\n",
    "    x2 = np.load(path + f'original/{i}/x2.npy')\n",
    "    x3 = np.load(path + f'original/{i}/x3.npy')\n",
    "\n",
    "    plt.imsave(path + f'original_png/x1_{i}.png', x1)\n",
    "    plt.imsave(path + f'original_png/x2_{i}.png', x2)\n",
    "    plt.imsave(path + f'original_png/x3_{i}.png', x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sequence_number):\n",
    "    x1 = plt.imread(path + f'out/x1_{i}_compressed.png')\n",
    "    x2 = plt.imread(path + f'out/x2_{i}_compressed.png')\n",
    "    x3 = plt.imread(path + f'out/x3_{i}_compressed.png')\n",
    "    \n",
    "    os.makedirs(save_path + f'FMD/{i}/', exist_ok=True)\n",
    "    np.save(path + f'FMD/{i}/x1.npy', x1)\n",
    "    np.save(path + f'FMD/{i}/x2.npy', x2)\n",
    "    np.save(path + f'FMD/{i}/x3.npy', x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "path = '/scratch/ssd004/scratch/joaodick/rebuttal_data/UVG/'\n",
    "for i in range(sequence_number):\n",
    "    with torch.no_grad():\n",
    "        x1 = torch.from_numpy(np.load(path + f'original/{i}/x1.npy')).to(device)\n",
    "        x2 = torch.from_numpy(np.load(path + f'original/{i}/x2.npy')).to(device)\n",
    "        x3 = torch.from_numpy(np.load(path + f'original/{i}/x3.npy')).to(device)\n",
    "        \n",
    "        #RUN INFERENCE\n",
    "        print(x1.shape)\n",
    "        break\n",
    "        '''\n",
    "        x1_hat = x1_hat.to(device)\n",
    "        x2_hat_JD = ssf_JD([x1_hat, x2])\n",
    "        x2_hat_AR = ssf_AR([x1_hat, x2])\n",
    "        x3_hat_JD = ssf_JD([x2_hat_JD, x3])\n",
    "        x3_hat_AR = ssf_AR([x2_hat_AR, x3])\n",
    "        \n",
    "        #TRANSFORM BACK TO IMG DOMAIN\n",
    "        x1_img = (hwc_tonp((resize(x1,(128,128))+1)*0.5))[0]\n",
    "        x2_img = (hwc_tonp((resize(x2,(128,128))+1)*0.5))[0]\n",
    "        x3_img = (hwc_tonp((resize(x3,(128,128))+1)*0.5))[0]\n",
    "        x1_hat_img = (hwc_tonp((resize(x1_hat,(128,128))+1)*0.5))[0]\n",
    "        x2_hat_JD_img = (hwc_tonp((resize(x2_hat_JD,(128,128))+1)*0.5))[0]\n",
    "        x2_hat_AR_img = (hwc_tonp((resize(x2_hat_AR,(128,128))+1)*0.5))[0]\n",
    "        x3_hat_JD_img = (hwc_tonp((resize(x3_hat_JD,(128,128))+1)*0.5))[0]\n",
    "        x3_hat_AR_img = (hwc_tonp((resize(x3_hat_AR,(128,128))+1)*0.5))[0]\n",
    "        #create directories\n",
    "        os.makedirs(save_path + f'original/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'JD/{i}/', exist_ok=True)\n",
    "        os.makedirs(save_path + f'AR/{i}/', exist_ok=True)\n",
    "        #save images\n",
    "        #original\n",
    "        np.save(save_path + f'original/{i}/x1.npy', x1_img)\n",
    "        np.save(save_path + f'original/{i}/x2.npy', x2_img)\n",
    "        np.save(save_path + f'original/{i}/x3.npy', x3_img)\n",
    "        #JD\n",
    "        np.save(save_path + f'JD/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'JD/{i}/x2.npy', x2_hat_JD_img)\n",
    "        np.save(save_path + f'JD/{i}/x3.npy', x3_hat_JD_img)\n",
    "        #AR\n",
    "        np.save(save_path + f'AR/{i}/x1.npy', x1_hat_img)\n",
    "        np.save(save_path + f'AR/{i}/x2.npy', x2_hat_AR_img)\n",
    "        np.save(save_path + f'AR/{i}/x3.npy', x3_hat_AR_img)\n",
    "        \n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
